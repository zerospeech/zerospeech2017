const char* QN_fltvec_bmul1_rcsid = "$Header: /u/drspeech/repos/quicknet2/QN_fltvec_bmul1.cc,v 1.2 2004/04/20 00:57:13 davidj Exp $";

// Floating point vector utility routines for QuickNet
// Bunch-mode floating point matrix ops

#include <assert.h>
#include "QN_fltvec.h"


/*
**
** Matrix-Matrix Code for the operation:
**    C = A*transpose(B) + C
**
** Automatically Generated by mm_gen ($Revision: 1.2 $) using the command:
**    /n/burrito/da/bilmes/phipac/mm_gen/mm_gen -cb 2 4 10 -cb 26 13 5 -file /tmp/dj -prec single -routine_name _mulntacc_mfmf_mf -alpha 1 -beta 1 -opB t 
**
** Generated on: Thursday June 22 1995, 20:36:55 PDT
** Created by: Jeff Bilmes <bilmes@cs.berkeley.edu>
**
*/

#define LOAD1x1(c00,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; \
}

#define STORE1x1(c00,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; \
}

/* Fixed M,K,N = 1,1,1 matrix matrix multiply. */
#define mul_mf1x1tmf1x1_mf1x1(c00,A0,B0) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B0[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; \
}


/* Fixed M,K,N = 1,2,1 matrix matrix multiply. */
#define mul_mf1x2tmf2x1_mf1x1(c00,A0,B0) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B0[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; \
   \
   _b0 = B0[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; \
}


/* Fixed M,K,N = 1,4,1 matrix matrix multiply. */
#define mul_mf1x4tmf4x1_mf1x1(c00,A0,B0) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B0[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; \
   \
   _b0 = B0[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; \
   \
   _b0 = B0[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; \
   \
   _b0 = B0[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; \
}


#define LOAD1x2(c00,c01,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; \
}

#define STORE1x2(c00,c01,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; \
}

/* Fixed M,K,N = 1,1,2 matrix matrix multiply. */
#define mul_mf1x1tmf1x2_mf1x2(c00,c01,A0,B0,B1) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
}


/* Fixed M,K,N = 1,2,2 matrix matrix multiply. */
#define mul_mf1x2tmf2x2_mf1x2(c00,c01,A0,B0,B1) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   \
   _b0 = B0[1]; _b1 = B1[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; \
}


/* Fixed M,K,N = 1,4,2 matrix matrix multiply. */
#define mul_mf1x4tmf4x2_mf1x2(c00,c01,A0,B0,B1) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   \
   _b0 = B0[1]; _b1 = B1[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   \
   _b0 = B0[2]; _b1 = B1[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   \
   _b0 = B0[3]; _b1 = B1[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; \
}


#define LOAD1x4(c00,c01,c02,c03,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; c02 = _cp[2]; c03 = _cp[3]; \
}

#define STORE1x4(c00,c01,c02,c03,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; _cp[2] = c02; _cp[3] = c03; \
}

/* Fixed M,K,N = 1,1,4 matrix matrix multiply. */
#define mul_mf1x1tmf1x4_mf1x4(c00,c01,c02,c03,A0,B0,B1,B2,B3) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
}


/* Fixed M,K,N = 1,2,4 matrix matrix multiply. */
#define mul_mf1x2tmf2x4_mf1x4(c00,c01,c02,c03,A0,B0,B1,B2,B3) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
}


/* Fixed M,K,N = 1,4,4 matrix matrix multiply. */
#define mul_mf1x4tmf4x4_mf1x4(c00,c01,c02,c03,A0,B0,B1,B2,B3) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   \
   _b0 = B0[2]; _b1 = B1[2]; _b2 = B2[2]; _b3 = B3[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   \
   _b0 = B0[3]; _b1 = B1[3]; _b2 = B2[3]; _b3 = B3[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
}


#define LOAD1x8(c00,c01,c02,c03,c04,c05,c06,c07,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; c02 = _cp[2]; c03 = _cp[3]; c04 = _cp[4]; c05 = _cp[5]; c06 = _cp[6]; c07 = _cp[7]; \
}

#define STORE1x8(c00,c01,c02,c03,c04,c05,c06,c07,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; _cp[2] = c02; _cp[3] = c03; _cp[4] = c04; _cp[5] = c05; _cp[6] = c06; _cp[7] = c07; \
}

/* Fixed M,K,N = 1,1,8 matrix matrix multiply. */
#define mul_mf1x1tmf1x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A0,B0,B1,B2,B3,B4,B5,B6,B7) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
}


/* Fixed M,K,N = 1,2,8 matrix matrix multiply. */
#define mul_mf1x2tmf2x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A0,B0,B1,B2,B3,B4,B5,B6,B7) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
}


/* Fixed M,K,N = 1,4,8 matrix matrix multiply. */
#define mul_mf1x4tmf4x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A0,B0,B1,B2,B3,B4,B5,B6,B7) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   \
   _b0 = B0[2]; _b1 = B1[2]; _b2 = B2[2]; _b3 = B3[2]; _b4 = B4[2]; _b5 = B5[2]; _b6 = B6[2]; _b7 = B7[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   \
   _b0 = B0[3]; _b1 = B1[3]; _b2 = B2[3]; _b3 = B3[3]; _b4 = B4[3]; _b5 = B5[3]; _b6 = B6[3]; _b7 = B7[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
}


#define LOAD1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; c02 = _cp[2]; c03 = _cp[3]; c04 = _cp[4]; c05 = _cp[5]; c06 = _cp[6]; c07 = _cp[7]; c08 = _cp[8]; c09 = _cp[9]; \
}

#define STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; _cp[2] = c02; _cp[3] = c03; _cp[4] = c04; _cp[5] = c05; _cp[6] = c06; _cp[7] = c07; _cp[8] = c08; _cp[9] = c09; \
}

/* Fixed M,K,N = 1,1,10 matrix matrix multiply. */
#define mul_mf1x1tmf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A0,B0,B1,B2,B3,B4,B5,B6,B7,B8,B9) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; _b8 = B8[0]; _b9 = B9[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
}


/* Fixed M,K,N = 1,2,10 matrix matrix multiply. */
#define mul_mf1x2tmf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A0,B0,B1,B2,B3,B4,B5,B6,B7,B8,B9) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; _b8 = B8[0]; _b9 = B9[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; _b8 = B8[1]; _b9 = B9[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
}


/* Fixed M,K,N = 1,4,10 matrix matrix multiply. */
#define mul_mf1x4tmf4x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A0,B0,B1,B2,B3,B4,B5,B6,B7,B8,B9) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; _b8 = B8[0]; _b9 = B9[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; _b8 = B8[1]; _b9 = B9[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   \
   _b0 = B0[2]; _b1 = B1[2]; _b2 = B2[2]; _b3 = B3[2]; _b4 = B4[2]; _b5 = B5[2]; _b6 = B6[2]; _b7 = B7[2]; _b8 = B8[2]; _b9 = B9[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   \
   _b0 = B0[3]; _b1 = B1[3]; _b2 = B2[3]; _b3 = B3[3]; _b4 = B4[3]; _b5 = B5[3]; _b6 = B6[3]; _b7 = B7[3]; _b8 = B8[3]; _b9 = B9[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
}


#define LOAD2x1(c00,c10,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; \
   _cp += Cstride; \
   c10 = _cp[0]; \
}

#define STORE2x1(c00,c10,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; \
   _cp += Cstride; \
   _cp[0] = c10; \
}

/* Fixed M,K,N = 2,1,1 matrix matrix multiply. */
#define mul_mf2x1tmf1x1_mf2x1(c00,c10,A0,A1,B0) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B0[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; \
   _a = A1[0]; \
   c10 += _a*_b0; \
}


/* Fixed M,K,N = 2,2,1 matrix matrix multiply. */
#define mul_mf2x2tmf2x1_mf2x1(c00,c10,A0,A1,B0) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B0[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; \
   _a = A1[0]; \
   c10 += _a*_b0; \
   \
   _b0 = B0[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; \
   _a = A1[1]; \
   c10 += _a*_b0; \
}


/* Fixed M,K,N = 2,4,1 matrix matrix multiply. */
#define mul_mf2x4tmf4x1_mf2x1(c00,c10,A0,A1,B0) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B0[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; \
   _a = A1[0]; \
   c10 += _a*_b0; \
   \
   _b0 = B0[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; \
   _a = A1[1]; \
   c10 += _a*_b0; \
   \
   _b0 = B0[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; \
   _a = A1[2]; \
   c10 += _a*_b0; \
   \
   _b0 = B0[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; \
   _a = A1[3]; \
   c10 += _a*_b0; \
}


#define LOAD2x2(c00,c01,c10,c11,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; \
   _cp += Cstride; \
   c10 = _cp[0]; c11 = _cp[1]; \
}

#define STORE2x2(c00,c01,c10,c11,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; \
   _cp += Cstride; \
   _cp[0] = c10; _cp[1] = c11; \
}

/* Fixed M,K,N = 2,1,2 matrix matrix multiply. */
#define mul_mf2x1tmf1x2_mf2x2(c00,c01,c10,c11,A0,A1,B0,B1) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; \
}


/* Fixed M,K,N = 2,2,2 matrix matrix multiply. */
#define mul_mf2x2tmf2x2_mf2x2(c00,c01,c10,c11,A0,A1,B0,B1) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   \
   _b0 = B0[1]; _b1 = B1[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
}


/* Fixed M,K,N = 2,4,2 matrix matrix multiply. */
#define mul_mf2x4tmf4x2_mf2x2(c00,c01,c10,c11,A0,A1,B0,B1) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   \
   _b0 = B0[1]; _b1 = B1[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   \
   _b0 = B0[2]; _b1 = B1[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[2]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   \
   _b0 = B0[3]; _b1 = B1[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A1[3]; \
   c10 += _a*_b0; c11 += _a*_b1; \
}


#define LOAD2x4(c00,c01,c02,c03,c10,c11,c12,c13,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; c02 = _cp[2]; c03 = _cp[3]; \
   _cp += Cstride; \
   c10 = _cp[0]; c11 = _cp[1]; c12 = _cp[2]; c13 = _cp[3]; \
}

#define STORE2x4(c00,c01,c02,c03,c10,c11,c12,c13,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; _cp[2] = c02; _cp[3] = c03; \
   _cp += Cstride; \
   _cp[0] = c10; _cp[1] = c11; _cp[2] = c12; _cp[3] = c13; \
}

/* Fixed M,K,N = 2,1,4 matrix matrix multiply. */
#define mul_mf2x1tmf1x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,A0,A1,B0,B1,B2,B3) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
}


/* Fixed M,K,N = 2,2,4 matrix matrix multiply. */
#define mul_mf2x2tmf2x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,A0,A1,B0,B1,B2,B3) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
}


/* Fixed M,K,N = 2,4,4 matrix matrix multiply. */
#define mul_mf2x4tmf4x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,A0,A1,B0,B1,B2,B3) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   \
   _b0 = B0[2]; _b1 = B1[2]; _b2 = B2[2]; _b3 = B3[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[2]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   \
   _b0 = B0[3]; _b1 = B1[3]; _b2 = B2[3]; _b3 = B3[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A1[3]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
}


#define LOAD2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; c02 = _cp[2]; c03 = _cp[3]; c04 = _cp[4]; c05 = _cp[5]; c06 = _cp[6]; c07 = _cp[7]; \
   _cp += Cstride; \
   c10 = _cp[0]; c11 = _cp[1]; c12 = _cp[2]; c13 = _cp[3]; c14 = _cp[4]; c15 = _cp[5]; c16 = _cp[6]; c17 = _cp[7]; \
}

#define STORE2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; _cp[2] = c02; _cp[3] = c03; _cp[4] = c04; _cp[5] = c05; _cp[6] = c06; _cp[7] = c07; \
   _cp += Cstride; \
   _cp[0] = c10; _cp[1] = c11; _cp[2] = c12; _cp[3] = c13; _cp[4] = c14; _cp[5] = c15; _cp[6] = c16; _cp[7] = c17; \
}

/* Fixed M,K,N = 2,1,8 matrix matrix multiply. */
#define mul_mf2x1tmf1x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,A0,A1,B0,B1,B2,B3,B4,B5,B6,B7) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
}


/* Fixed M,K,N = 2,2,8 matrix matrix multiply. */
#define mul_mf2x2tmf2x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,A0,A1,B0,B1,B2,B3,B4,B5,B6,B7) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
}


/* Fixed M,K,N = 2,4,8 matrix matrix multiply. */
#define mul_mf2x4tmf4x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,A0,A1,B0,B1,B2,B3,B4,B5,B6,B7) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   \
   _b0 = B0[2]; _b1 = B1[2]; _b2 = B2[2]; _b3 = B3[2]; _b4 = B4[2]; _b5 = B5[2]; _b6 = B6[2]; _b7 = B7[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[2]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   \
   _b0 = B0[3]; _b1 = B1[3]; _b2 = B2[3]; _b3 = B3[3]; _b4 = B4[3]; _b5 = B5[3]; _b6 = B6[3]; _b7 = B7[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A1[3]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
}


#define LOAD2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,C,Cstride) \
{\
   float * _cp = C; \
   c00 = _cp[0]; c01 = _cp[1]; c02 = _cp[2]; c03 = _cp[3]; c04 = _cp[4]; c05 = _cp[5]; c06 = _cp[6]; c07 = _cp[7]; c08 = _cp[8]; c09 = _cp[9]; \
   _cp += Cstride; \
   c10 = _cp[0]; c11 = _cp[1]; c12 = _cp[2]; c13 = _cp[3]; c14 = _cp[4]; c15 = _cp[5]; c16 = _cp[6]; c17 = _cp[7]; c18 = _cp[8]; c19 = _cp[9]; \
}

#define STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] = c00; _cp[1] = c01; _cp[2] = c02; _cp[3] = c03; _cp[4] = c04; _cp[5] = c05; _cp[6] = c06; _cp[7] = c07; _cp[8] = c08; _cp[9] = c09; \
   _cp += Cstride; \
   _cp[0] = c10; _cp[1] = c11; _cp[2] = c12; _cp[3] = c13; _cp[4] = c14; _cp[5] = c15; _cp[6] = c16; _cp[7] = c17; _cp[8] = c18; _cp[9] = c19; \
}

/* Fixed M,K,N = 2,1,10 matrix matrix multiply. */
#define mul_mf2x1tmf1x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,A0,A1,B0,B1,B2,B3,B4,B5,B6,B7,B8,B9) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; _b8 = B8[0]; _b9 = B9[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
}


/* Fixed M,K,N = 2,2,10 matrix matrix multiply. */
#define mul_mf2x2tmf2x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,A0,A1,B0,B1,B2,B3,B4,B5,B6,B7,B8,B9) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; _b8 = B8[0]; _b9 = B9[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; _b8 = B8[1]; _b9 = B9[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
}


/* Fixed M,K,N = 2,4,10 matrix matrix multiply. */
#define mul_mf2x4tmf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,A0,A1,B0,B1,B2,B3,B4,B5,B6,B7,B8,B9) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B0[0]; _b1 = B1[0]; _b2 = B2[0]; _b3 = B3[0]; _b4 = B4[0]; _b5 = B5[0]; _b6 = B6[0]; _b7 = B7[0]; _b8 = B8[0]; _b9 = B9[0]; \
   _a = A0[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[0]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   \
   _b0 = B0[1]; _b1 = B1[1]; _b2 = B2[1]; _b3 = B3[1]; _b4 = B4[1]; _b5 = B5[1]; _b6 = B6[1]; _b7 = B7[1]; _b8 = B8[1]; _b9 = B9[1]; \
   _a = A0[1]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   \
   _b0 = B0[2]; _b1 = B1[2]; _b2 = B2[2]; _b3 = B3[2]; _b4 = B4[2]; _b5 = B5[2]; _b6 = B6[2]; _b7 = B7[2]; _b8 = B8[2]; _b9 = B9[2]; \
   _a = A0[2]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[2]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   \
   _b0 = B0[3]; _b1 = B1[3]; _b2 = B2[3]; _b3 = B3[3]; _b4 = B4[3]; _b5 = B5[3]; _b6 = B6[3]; _b7 = B7[3]; _b8 = B8[3]; _b9 = B9[3]; \
   _a = A0[3]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A1[3]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
}


/* Fixed M,N = 52,50, Arbitrary K L1-blocked matrix matrix multiply. */
void
_mulntacc_mfmf_mf_l1_arb_k(int K, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0_0,*ap0_1;
   const float *bp0_0,*bp0_1,*bp0_2,*bp0_3,*bp0_4,*bp0_5,*bp0_6,*bp0_7,*bp0_8,*bp0_9;
   float *cp0;
   const int A_sbs_stride = Astride*2;
   const int B_sbs_stride = Bstride*10;
   const int C_sbs_stride = Cstride*2;
   const int k_marg_el = K & 3;
   const int k_norm = K - k_marg_el;
   float *const c0_endp = C+52*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=A_sbs_stride) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + 50;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=10) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         bp0_4 = bp0_3 + Bstride;
         bp0_5 = bp0_4 + Bstride;
         bp0_6 = bp0_5 + Bstride;
         bp0_7 = bp0_6 + Bstride;
         bp0_8 = bp0_7 + Bstride;
         bp0_9 = bp0_8 + Bstride;
         LOAD2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4,bp0_4+=4,bp0_5+=4,bp0_6+=4,bp0_7+=4,bp0_8+=4,bp0_9+=4) {
            mul_mf2x4tmf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         if (k_marg_el & 0x2) {
            mul_mf2x2tmf2x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
            ap0_0+=2;ap0_1+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;bp0_4+=2;bp0_5+=2;bp0_6+=2;bp0_7+=2;bp0_8+=2;bp0_9+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf2x1tmf1x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
      }
   }
}

/* Arbitrary M,K,N L1-blocked matrix matrix multiply. */
void
_mulntacc_mfmf_mf_l1_arb_all(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0_0,*ap0_1;
   const float *bp0_0,*bp0_1,*bp0_2,*bp0_3,*bp0_4,*bp0_5,*bp0_6,*bp0_7,*bp0_8,*bp0_9;
   float *cp0;
   const int A_sbs_stride = Astride*2;
   const int B_sbs_stride = Bstride*10;
   const int C_sbs_stride = Cstride*2;
   const int k_marg_el = K & 3;
   const int k_norm = K - k_marg_el;
   const int m_marg_el = M & 1;
   const int m_norm = M - m_marg_el;
   const int n_marg_el = N % 10;
   const int n_norm = N - n_marg_el;
   float *const c0_endp = C+m_norm*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=A_sbs_stride) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=10) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         bp0_4 = bp0_3 + Bstride;
         bp0_5 = bp0_4 + Bstride;
         bp0_6 = bp0_5 + Bstride;
         bp0_7 = bp0_6 + Bstride;
         bp0_8 = bp0_7 + Bstride;
         bp0_9 = bp0_8 + Bstride;
         LOAD2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4,bp0_4+=4,bp0_5+=4,bp0_6+=4,bp0_7+=4,bp0_8+=4,bp0_9+=4) {
            mul_mf2x4tmf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         if (k_marg_el & 0x2) {
            mul_mf2x2tmf2x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
            ap0_0+=2;ap0_1+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;bp0_4+=2;bp0_5+=2;bp0_6+=2;bp0_7+=2;bp0_8+=2;bp0_9+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf2x1tmf1x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
      }
   }
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=A_sbs_stride) {
      const float* const ap0_endp = a0 + k_norm;
      b0 = B+n_norm*Bstride;
      cp0 = c0+n_norm;
      if (n_marg_el & 0x8) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         bp0_4 = bp0_3 + Bstride;
         bp0_5 = bp0_4 + Bstride;
         bp0_6 = bp0_5 + Bstride;
         bp0_7 = bp0_6 + Bstride;
         LOAD2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4,bp0_4+=4,bp0_5+=4,bp0_6+=4,bp0_7+=4) {
            mul_mf2x4tmf4x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7);
         }
         if (k_marg_el & 0x2) {
            mul_mf2x2tmf2x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7);
            ap0_0+=2;ap0_1+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;bp0_4+=2;bp0_5+=2;bp0_6+=2;bp0_7+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf2x1tmf1x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7);
         }
         STORE2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,cp0,Cstride);
         b0 += Bstride*8;
         cp0 += 8;
      }
      if (n_marg_el & 0x4) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         LOAD2x4(c00,c01,c02,c03,c10,c11,c12,c13,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4) {
            mul_mf2x4tmf4x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3);
         }
         if (k_marg_el & 0x2) {
            mul_mf2x2tmf2x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3);
            ap0_0+=2;ap0_1+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf2x1tmf1x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3);
         }
         STORE2x4(c00,c01,c02,c03,c10,c11,c12,c13,cp0,Cstride);
         b0 += Bstride*4;
         cp0 += 4;
      }
      if (n_marg_el & 0x2) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         LOAD2x2(c00,c01,c10,c11,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4,bp0_1+=4) {
            mul_mf2x4tmf4x2_mf2x2(c00,c01,c10,c11,ap0_0,ap0_1,bp0_0,bp0_1);
         }
         if (k_marg_el & 0x2) {
            mul_mf2x2tmf2x2_mf2x2(c00,c01,c10,c11,ap0_0,ap0_1,bp0_0,bp0_1);
            ap0_0+=2;ap0_1+=2;
            bp0_0+=2;bp0_1+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf2x1tmf1x2_mf2x2(c00,c01,c10,c11,ap0_0,ap0_1,bp0_0,bp0_1);
         }
         STORE2x2(c00,c01,c10,c11,cp0,Cstride);
         b0 += Bstride*2;
         cp0 += 2;
      }
      if (n_marg_el & 0x1) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         LOAD2x1(c00,c10,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4) {
            mul_mf2x4tmf4x1_mf2x1(c00,c10,ap0_0,ap0_1,bp0_0);
         }
         if (k_marg_el & 0x2) {
            mul_mf2x2tmf2x1_mf2x1(c00,c10,ap0_0,ap0_1,bp0_0);
            ap0_0+=2;ap0_1+=2;
            bp0_0+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf2x1tmf1x1_mf2x1(c00,c10,ap0_0,ap0_1,bp0_0);
         }
         STORE2x1(c00,c10,cp0,Cstride);
      }
   }
   if (m_marg_el & 0x1) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=10) {
         ap0_0 = a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         bp0_4 = bp0_3 + Bstride;
         bp0_5 = bp0_4 + Bstride;
         bp0_6 = bp0_5 + Bstride;
         bp0_7 = bp0_6 + Bstride;
         bp0_8 = bp0_7 + Bstride;
         bp0_9 = bp0_8 + Bstride;
         LOAD1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4,bp0_4+=4,bp0_5+=4,bp0_6+=4,bp0_7+=4,bp0_8+=4,bp0_9+=4) {
            mul_mf1x4tmf4x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         if (k_marg_el & 0x2) {
            mul_mf1x2tmf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
            ap0_0+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;bp0_4+=2;bp0_5+=2;bp0_6+=2;bp0_7+=2;bp0_8+=2;bp0_9+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf1x1tmf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,cp0,Cstride);
      }
      if (n_marg_el & 0x8) {
         ap0_0 = a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         bp0_4 = bp0_3 + Bstride;
         bp0_5 = bp0_4 + Bstride;
         bp0_6 = bp0_5 + Bstride;
         bp0_7 = bp0_6 + Bstride;
         LOAD1x8(c00,c01,c02,c03,c04,c05,c06,c07,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4,bp0_4+=4,bp0_5+=4,bp0_6+=4,bp0_7+=4) {
            mul_mf1x4tmf4x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7);
         }
         if (k_marg_el & 0x2) {
            mul_mf1x2tmf2x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7);
            ap0_0+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;bp0_4+=2;bp0_5+=2;bp0_6+=2;bp0_7+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf1x1tmf1x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7);
         }
         STORE1x8(c00,c01,c02,c03,c04,c05,c06,c07,cp0,Cstride);
         b0 += Bstride*8;
         cp0 += 8;
      }
      if (n_marg_el & 0x4) {
         ap0_0 = a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         LOAD1x4(c00,c01,c02,c03,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4) {
            mul_mf1x4tmf4x4_mf1x4(c00,c01,c02,c03,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3);
         }
         if (k_marg_el & 0x2) {
            mul_mf1x2tmf2x4_mf1x4(c00,c01,c02,c03,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3);
            ap0_0+=2;
            bp0_0+=2;bp0_1+=2;bp0_2+=2;bp0_3+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf1x1tmf1x4_mf1x4(c00,c01,c02,c03,ap0_0,bp0_0,bp0_1,bp0_2,bp0_3);
         }
         STORE1x4(c00,c01,c02,c03,cp0,Cstride);
         b0 += Bstride*4;
         cp0 += 4;
      }
      if (n_marg_el & 0x2) {
         ap0_0 = a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         LOAD1x2(c00,c01,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,bp0_0+=4,bp0_1+=4) {
            mul_mf1x4tmf4x2_mf1x2(c00,c01,ap0_0,bp0_0,bp0_1);
         }
         if (k_marg_el & 0x2) {
            mul_mf1x2tmf2x2_mf1x2(c00,c01,ap0_0,bp0_0,bp0_1);
            ap0_0+=2;
            bp0_0+=2;bp0_1+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf1x1tmf1x2_mf1x2(c00,c01,ap0_0,bp0_0,bp0_1);
         }
         STORE1x2(c00,c01,cp0,Cstride);
         b0 += Bstride*2;
         cp0 += 2;
      }
      if (n_marg_el & 0x1) {
         ap0_0 = a0;
         bp0_0 = b0;
         LOAD1x1(c00,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,bp0_0+=4) {
            mul_mf1x4tmf4x1_mf1x1(c00,ap0_0,bp0_0);
         }
         if (k_marg_el & 0x2) {
            mul_mf1x2tmf2x1_mf1x1(c00,ap0_0,bp0_0);
            ap0_0+=2;
            bp0_0+=2;
         }
         if (k_marg_el & 0x1) {
            mul_mf1x1tmf1x1_mf1x1(c00,ap0_0,bp0_0);
         }
         STORE1x1(c00,cp0,Cstride);
      }
   }
}

/* Fixed M,K,N = 52,52,50 L1-blocked matrix matrix multiply. */
void
_mulntacc_mfmf_mf_l1(const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0_0,*ap0_1;
   const float *bp0_0,*bp0_1,*bp0_2,*bp0_3,*bp0_4,*bp0_5,*bp0_6,*bp0_7,*bp0_8,*bp0_9;
   float *cp0;
   const int A_sbs_stride = Astride*2;
   const int B_sbs_stride = Bstride*10;
   const int C_sbs_stride = Cstride*2;
   float *const c0_endp = C+52*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=A_sbs_stride) {
      const float* const ap0_endp = a0 + 52;
      float* const cp0_endp = c0 + 50;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=10) {
         ap0_0 = a0;
         ap0_1 = ap0_0 + Astride;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         bp0_2 = bp0_1 + Bstride;
         bp0_3 = bp0_2 + Bstride;
         bp0_4 = bp0_3 + Bstride;
         bp0_5 = bp0_4 + Bstride;
         bp0_6 = bp0_5 + Bstride;
         bp0_7 = bp0_6 + Bstride;
         bp0_8 = bp0_7 + Bstride;
         bp0_9 = bp0_8 + Bstride;
         LOAD2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
         for (; ap0_0!=ap0_endp; ap0_0+=4,ap0_1+=4,bp0_0+=4,bp0_1+=4,bp0_2+=4,bp0_3+=4,bp0_4+=4,bp0_5+=4,bp0_6+=4,bp0_7+=4,bp0_8+=4,bp0_9+=4) {
            mul_mf2x4tmf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0_0,ap0_1,bp0_0,bp0_1,bp0_2,bp0_3,bp0_4,bp0_5,bp0_6,bp0_7,bp0_8,bp0_9);
         }
         STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
      }
   }
}

void
qn_pp_mulntacc_mfmf_mf_sss(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride)
{
   /* Code for L2-blocked routine. */
   int m2,k2,n2;
   const float *a2,*b2;
   float *c2;
   const float *ap2,*bp2;
   float *cp2;
   if (M < 53 && K < 53 && N < 51) {
      _mulntacc_mfmf_mf_l1_arb_all(M,K,N,A,B,C,Astride,Bstride,Cstride);
      return;
   }
   for (m2=0; m2<=M-52; m2+=52) {
      c2 = C + m2*Cstride;
      a2 = A + m2*Astride;
      for (n2=0,b2=B,cp2=c2; n2<=N-50; n2+=50,b2+=50*Bstride,cp2+=50) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52,ap2+=52) {
            _mulntacc_mfmf_mf_l1(ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
         if (k2 < K) {
            _mulntacc_mfmf_mf_l1_arb_k(K-k2,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52,ap2+=52) {
            _mulntacc_mfmf_mf_l1_arb_all(52,52,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
         if (k2 < K) {
            _mulntacc_mfmf_mf_l1_arb_all(52,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
      }
   }
   if (m2 < M) {
      c2 = C + m2*Cstride;
      a2 = A + m2*Astride;
      for (n2=0,b2=B,cp2=c2; n2<=N-50; n2+=50,b2+=50*Bstride,cp2+=50) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52,ap2+=52) {
            _mulntacc_mfmf_mf_l1_arb_all(M-m2,52,50,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
         if (k2 < K) {
            _mulntacc_mfmf_mf_l1_arb_all(M-m2,K-k2,50,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52,ap2+=52) {
            _mulntacc_mfmf_mf_l1_arb_all(M-m2,52,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
         if (k2 < K) {
            _mulntacc_mfmf_mf_l1_arb_all(M-m2,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride);
         }
      }
   }
}


