. $Header: /u/drspeech/repos/quicknet2/qnmultitrn.man,v 1.14 2011/05/20 23:29:30 davidj Exp $
.TH qnmultitrn 1 "$Date: 2011/05/20 23:29:30 $" ICSI "ICSI SPEECH SOFTWARE"
.SH NAME
qnmultitrn \- multi-layer MLP training program
.SH SYNOPSIS
.B qnmultitrn
[
.I options
]
.SH DESCRIPTION
.I qnmultitrn
is a program for training MLPs (Multi Layer Perceptrons - a.k.a.
Neural Nets).  In particular, it is useful for training MLPs for
speech recognition and other signal processing tasks.  Essentially, it
takes one or more files of training data and produces a file
containing the weights of an appropriately trained MLP.
.P
.I qnmultitrn
differs from the older
.I qnstrn
in various ways, including:
.RS
.I qnmultitrn
supports MLPs with 2 to 5 layers, rather than just 3.
.P
Various command-lines arguments have been changed for consistency.
.P
New file formats: matlab weights.
.PD 1
.SH OPTIONS
.I qnmultitrn
can take the following options:
.P
.PD 0
.BI ftr1_file= filename1
.TP
.PD 1
.BI ftr2_file= filename2
Specify the files containing feature information for training the
net.  The features in each file are normalized, selected (to eliminate
unwanted features) and windowed separately, before being combined to
form the input to the MLP.  If \fBftr2_file\fR is the empty string (the
default), only one feature file is used.
.P
.PD 0
.BI ftr1_format= format
.TP
.PD 1
.BI ftr2_format= format
Specify the format of the respective feature files.  The format can be
\fBpfile\fR (the ICSI feature file format - see \fBpfile\fR(5)) or
\fBpre\fR (the Cambridge compressed feature file format).  The
default is \fBpfile\fR.
.P
.PD 0
.BI ftr1_width= integer
.TP
.PD 1
.BI ftr2_width= integer
Specify the number of features in the respective feature files.  The
default \- \fB0\fR
\- means use the number of features specified in the feature file header.
A non-zero value must be given for feature files that are in a format
that does not include a header, e.g. \fBpre\fR files.
.TP
.BI unary_file= filename
Specify the pfile containing label information for providing a one-hot
unary input to the net.  This is typically used to supply previous
state input to the net, in which case the value is the same as the
\fBhardtarget_file\fR option.
.TP
.BI hardtarget_file= filename
Specify the pfile containing label information for specifying the
target when training the net.  This can be the same as either or both
of the ftr_files.  The target file must contain only one label per
frame. One and only one of the \fBhardtarget_file\fR and
\fBsofttarget_file\fR options can be non-null.
.TP
.BI hardtarget_format= format
Specify the format of the files containing target labels.  The format can be
\fBpfile\fR (the ICSI feature file format - see \fBpfile\fR(5)),
\fBpre\fR (the Cambridge compressed feature file format) or 
\fBilab\fR (the ICSI compressed label file format).  The
default is \fBpfile\fR.  If the \fBpre\fR format is specified
\fBhardtarget_file\fR and \fBftr1_file\fR must be the same.
.TP
.BI softtarget_file= filename
Specify the file containing target vectors to use when training
the net. There must be the same number of features in each frame as
there are outputs from the net.  One and only one of the
\fBhardtarget_file\fR and \fBsofttarget_file\fR options can be
non-null.
.TP
.BI softtarget_format= format
Specify the format of the target vector file.  The format can be
\fBpfile\fR (the ICSI feature file format - see \fBpfile\fR(5)) or
\fBlna\fR (the Cambridge compressed activation file format).  The
default is \fBpfile\fR.
.TP
.BI softtarget_width= integer
Specify the number of features in the target vector file.  The
default \- \fB0\fR
\- means use the number of features specified in the feature file header.
A non-zero value must be given for feature files that are in a format
that does not include a header, e.g. \fBpre\fR files.  Currently, the
effective width must be the same as the number of output units.
.P
.PD 0
.BI ftr1_norm_file= filename1
.TP
.PD 1
.BI ftr2_norm_file= filename2
Specify a file containing normalization parameters for the values in
the corresponding feature files.  The normalization values are used
for both training and cross validation.  A null string results in no
normalization.  See \fBftr1_norm_mode\fR below.
.P
.PD 0
.BI ftr1_ftr_start= integer
.TP
.PD 1
.BI ftr2_ftr_start= integer
Specify the column number of the first feature to use from the
corresponding feature file (possibly augmented by delta_order).  
\fB0\fR means start with the first
feature column.
.P
.PD 0
.BI ftr1_ftr_count= integer
.TP
.PD 1
.BI ftr2_ftr_count= integer
Specify the number of feature to use from the corresponding feature
file (including added feature columns).  
The default \- \fB0\fR means use all remaining features.
.TP
.BI hardtarget_lastlab_reject= bool
Usually if the net has \fIn\fR outputs, then the valid range of hardtarget
label values is \fI0\fR through \fIn - 1\fR.  However, if
\fBhardtarget_lastlab_reject\fR is set to \fBtrue\fR then an extra
label value is allowed.  A label of value \fBn\fR will mean "do not
train on this label".  This prevents the features from being presented
to the net and also means the frame will be ignored when
calculating cross-validation accuracy.
.TP
.BI window_extent= integer
Specify the number of frames  from the beginning of the first input
window to the end of the last input window.  Typically this is the
same as \fBftr1_window_len\fR.
.P
.PD 0
.BI ftr1_window_offset= integer
.TP
.PD 1
.BI ftr2_window_offset= integer
Specify the offset, in frames, of the window on each feature file from
the base of the overall window.  Typically,
\fBftr1_window_offset\fR is \fB0\fR.
.TP
.BI unary_window_offset= integer
Specify the offset, in frames, of the window on the unary input file from
the base of the overall window.  Note that the length of the unary input
file window is always \fB1\fR for normal hard training.
.TP
.BI hardtarget_window_offset= integer
Specify the offset, in frames, of the window on the label file from
the base of the overall window.  Note that the length of the label
file window is always \fB1\fR for normal hard training.
.TP
.BI softtarget_window_offset= integer
Specify the offset, in frames, of the window on the soft targget file from
the base of the overall window.  Note that the length of the soft target
window is currently assumed to be \fB1\fR.
.P
.PD 0
.BI ftr1_window_len= integer
.TP
.PD 1
.BI ftr2_window_len= integer
Specify the length, in frames, of the window on each feature file.
Often, the value of \fBftr1_window_len\fR will be the same as
the value of \fBwindow_extent\fR.
.P
.PD 0
.BI ftr1_delta_order= integer
.TP
.PD 1
.BI ftr2_delta_order= integer
Specify the order of delta-calculations to be applied to each 
feature file.  Currently, this is applied 
.I before
feature selection 
via \fBftr_start\fR and \fBftr_count\fR, effectively 
doubling or tripling the width of the 
input feature stream.  \fBftr_start\fR and \fBftr_count\fR 
can then be used to select columns from the net aggregate feature set.
Acceptable values are 0 (no action), 1 (just deltas) 
and 2 (deltas and double-deltas). 
Note that the filters are not scaled by the sum of squares, so
mean-variance normalization is recommended.
.P
.PD 0
.BI ftr1_delta_win= integer
.TP
.PD 1
.BI ftr2_delta_win= integer
Number of points in the FIR discrete-differentiators used to 
calculate deltas and double deltas.  Default value is 9.
.P
.PD 0
.BI ftr1_norm_mode= mode
.TP
.PD 1
.BI ftr2_norm_mode= mode
Specifies normalizataion applied to each pfile as 
"file", "utts" or "online", for constant normalization, 
per-utterance normalization (in which case the norm_file 
is ignored), or online normalization (reset to values from 
the norm file at the start of each utterance).  Setting e.g.
\fBftr1_norm_mode\fR to "file" and setting \fBftr1_norm_file\fR to the
null string results in no normalization.  See 
qncopy(1) for more description.  Note that per-utterance 
normalization involves reading each utterance twice, 
which can incur considerable frame throughput penalties.
.P
.PD 0
.BI ftr1_norm_alpha_m= val
.P
.BI ftr2_norm_alpha_m= val
.P
.BI ftr1_norm_alpha_v= val
.TP
.PD 1
.BI ftr2_norm_alpha_v= val
Update constants for the mean and variance estimates for the online 
normalization (if selected) for the two feature streams.  Default is 
0.005.  See qncopy(1).
.TP
.BI train_cache_frames= integer
The number of frames of training data loaded into the cache used for
presentation randomization.  A variable number of sentences is read
sequentially from the training feature file until this cache is as
close to being full as possible.  This cache is then used to provide
random presentations for training until all frames have been used
once, at which point the next sequence of sentences is loaded.  The
number of frames cached has a significant affect on memory usage, with
one frame requiring approximately (ftr_count+lab_count+1)*4 bytes.
.TP
.BI train_cache_seed= integer
Set the seed for random training pattern selection.
.TP
.BI train_sent_range= range-spec-string
The sentences to use for training, specified by a Range token in 
one of the formats defined by QN_Range(3).  
.TP
.BI cv_sent_range= range-spec-string
The sentences to use for cross validation, specified by a Range token in 
one of the formats defined by QN_Range(3).
.TP
.BI out_weight_file= filename
Specify a file in which to save the weights from the trained net.
.TP
.BI log_weight_file= filename
Specify a file in which a copy of the weights are saved at the end of
each epoch.  This file is used when the cross validation error rate for an
epoch is greater than the previous one, in which case the latest weight
updates are abandoned by reloading the weights from this file. See
below for details on using % characters in weight file names.
.TP
.BI ckpt_weight_file= filename
Specify a file in which periodic or signal-triggerd checkpoints of
the current weights are saved.
.P
Five special substrings can be used in the log_ and ckpt_
weightfile filenames. The string
.B %p
is replaced by the process number of the qnstrn process.  The string
.B %e
is replaced by the number of the current epoch (allowing weight
logs from all epochs to be retained).
The string
.B %t
is replaced by the date and time as would be produced by the Unix
command \fBdate +'%Y%m%d-%H:%M\fR.
The string
.B %h
is replaced by the hostname of the running machine with all 
characters from the first period removed.
.B %%
is replaced by a single
.BR % "."
.TP
.BI init_weight_file= filename
Specifiy a file containing weights to load into the net before
training.  Specifying an empty string as the filename
means use random initial weights.
.TP
.BI out_weight_format= format
.TP
.BI log_weight_format= fromat
.TP
.BI ckpt_weight_format= format
.TP
.BI init_weight_format= format
Specify the format of the respective weight files.  The format can be
\fBmatlab\fR (encoded as old-style Matlab format float matrices) or
\fBrap3\fR (the orignal RAP and qnstrn weight file format that only
works for 3 layer MLPs).  The
default is \fBmatlab\fR.
.P
.PD 0
.BI init_random_bias_min= float[,float][,float...]
.P
.BI init_random_bias_max= float[,float][,float...]
.P
.BI init_random_weight_min= float[,float][,float...]
.TP
.PD 1
.BI init_random_weight_max= float,[float][,float...]
Limits for the random initialization of weights and biases \- only
used if \fBinput_weight_file\fR is the null string.  If one value
supplied, applies to all weights/biases.  If two values supplied,
second applies to the output layer and the first to the rest of the
layers.  If more than two values
are supplied, there must be one fewer values than the number of layers
(due to the input layer not having biases or weights).  In this case, values
apply to layers in order of input through output.
.TP
.BI init_random_seed= integer
Set the seed for random number generation used for
initialization of weights.
.TP
.BI learnrate_schedule= schedule
The name of the strategy used for updating the learning rate in
successive training epochs.

\fInewbob\fR means use the same approach as taken by BoB: use a
constant learning rate until the error reduction drops below a given
threshold, then decrease exponentially.  The initial learning rate is
set the first value specified in the \fBlearnrate_vals\fR option.  The
ratio of successive learning rates when decaying is set by
\fRlearnrate_scale\fB. 

\fIlist\fR means use the learning rate list specified in the
\fBlearnrate_vals\fR option.  After all learning rates have been used, the
training terminates.

The default value is \fInewbob\fB.
.TP
.BI learnrate_vals= float,float,...
Specify the learning rate for one or more epochs.  See the
\fBlearnrate_schedule\fR option for more details.
.TP
.BI learnrate_scale= float
Specify the scaling factor of new learning rates to old learning
rates during learning rate decay.  See
\fBlearnrate_schedule\fR option for more details.
.TP
.BI learnrate_epochs= integer
Maximum number of epochs of training.  We stop when we have trained
this many epochs regardless of the learning rate schedule.  if
\fBlearnrate_epochs\fR is 0, we do not train at all.
.TP
.BI unary_size= integer
The number of inputs to the MLP that use a one-high encoding based on
the labels in \fBunary_file\fR.  For previous state training, this
should be set to the number of states.  The default value \- \fB0\fR \-
disables the unary input to the net.  Note that when using a unary
input for training with previous state, the results from cross
validation may well not mean much.
.TP
.BI mlp_size= integer,integer[,...]
Specify the size of the MLP layers, input layer first.  The number of
integers implies the number of layers, which can be between 2 and 5.
.TP
.BI mlp_output_type= unittype
Specify the type of non-linearity to use for the MLP output layer.
Allowable values are \fBsigmoid\fR, \fBsigmoidx\fR (sigmoid with cross
entropy error criteria during training), \fBtanh\fR and \fBsoftmax\fR
(the default). 
.TP
.BI mlp_lrmultiplier= float[,...]
Specify scaling values for the learning rates in individual sections
in the MLP. 
This is either one value, which applies to all weight and bias
learning rates (not very useful), or a list of values that is of
length one less than the number 
of layers.  E.g on a 3 layer MLPs, the first value scales the
input-to-hidden and hidden bias learning rates and the second value
scales the hidden-to-ouput and output bias learning rates.  Values
of 0.0 on individual sections is optimized as a special case, with
arithmetic operations being skipped rather than adding zeros.
.TP
.BI use_pp= bool
Use high-performance internal matrix routines for the MLP if
.BR true .
This is enabled by default and turning it off is
only really useful for debugging or
performance tuning.  Note that the transcendental routines are only
approximations and so there are slight numerical differences in the
result depending on how this option is set.
.TP 
.BI use_blas= bool
Use blas matrix routines for the MLP if
.BR true .
Setting this to true will generate an error if there is no BLAS
library linked with the executable.  Note that having
use_blas and use_pp both true is reasonable as some internal
optimized routines are not available in the BLAS library.
.TP 
.BI use_fe= bool
Use fast exponent approximation for e.g. sigmoid and softmax functions
if
.BR true .
There are some situations where the fast exponent approximation has
been shown to reduce training accuracy.  However, not using fast
exponent approximation off can result in a lower
connection-updates-per-second number.  Fast exponent is not available on CUDA
hardware and the setting will be ignored.  As of QuickNet v3_31, the
default is
.BR false .
In earlier versions the default was
.BR true .
.TP 
.BI use_cuda= bool
Use any available CUDA hardware if
.BR true .
If no CUDA hardware is available, the program will halt.
.TP 
.BI mlp_bunch_size= integer
Set the number of presentations between forward and backward phases
during MLP training.  A value of \fB0\fR (the default) results in
traditional online training.  \fB1\fR means to use the bunch-mode 
routines, but with a bunch size of 1, which should be numerically very
similar to online training but may have different speed
characteristics.  The default value is 16, which will work well for
most datasets.  Larger bunch sizes will typically result in higher
connection update rates but beyond a dataset-specific limit the convergence
characteristics of the net may be affected.
.TP
.BI mlp_threads= integer
Set the number of CPU threads to use.  Note that this only works for a
bunch size >1. For good performance, the number of threads should be
a small fraction of the bunch size and less than or equal to the
number of available physical cores.
.TP
.BI log_file= filename
The file in which to log status messages.  Specifying a
filename of
.B \-
sends the results to standard output.
.TP
.BI verbose= bool
Output more status messages if
.BR true .
.TP
.BI debug= integer
Set the level of internal debugging output.  \fB0\fR means none,
\fB6\fR means lots!
.TP

.SH ENVIRONMENT
.TP 20
.B TZ
Time Zone.  On some systems, this is used for displaying times during
the training run \- if times seem to be wrong by several hours, it is
because this environment variable is not set.

.SH AUTHOR
David Johnson  <davidj@ICSI.Berkeley.EDU>
.SH SEE ALSO
.BR qnmultifwd (1) ,
.BR qnstrn (1) ,
.BR qnsfwd (1) ,
.BR qnnorm (1) ,
.BR qncopy (1) ,
.BR norms (5) ,
.BR pfile (5) ,
.BR ilab (5)
.SH BUGS




